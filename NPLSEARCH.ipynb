{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to genearte an NLP Based search engine on specific documents \n",
    "\n",
    "Data is from Kaggle on products and the code combined several fields as descriptor for the product\n",
    "\n",
    "Then the NLP process ahave a search option to produce the closest matches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jorgepinzon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jorgepinzon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     /Users/jorgepinzon/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/jorgepinzon/.cache/kagglehub/datasets/sujaykapadnis/products-datasets/versions/1\n",
      "Path to dataset files: /Users/jorgepinzon/.cache/kagglehub/datasets/promptcloud/amazon-product-dataset-2020/versions/1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14568"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import pandas as pd \n",
    "import nltk\n",
    "# Download NLTK data (run once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('names')\n",
    "# Download latest version of this dataset: https://www.kaggle.com/datasets/sujaykapadnis/products-datasets\n",
    "path = kagglehub.dataset_download(\"sujaykapadnis/products-datasets\")\n",
    "documents = {}\n",
    "print(\"Path to dataset files:\", path)\n",
    "try: \n",
    "    prod_data = pd.read_csv(path+os.listdir(path)[0])\n",
    "    for index, row in prod_data.iterrows():\n",
    "        documents[row['S.No']] = row['Product Name']+\" \"+row['Brand Desc']+\" \"+row['Product Size']+\" \"+row['Category']\n",
    "except:\n",
    "    path = '/Users/jorgepinzon/.cache/kagglehub/datasets/sujaykapadnis/products-datasets/versions/1/'\n",
    "    prod_data = pd.read_csv(path+os.listdir(path)[0])\n",
    "    for index, row in prod_data.iterrows():\n",
    "        documents[row['S.No']] = row['Product Name']+\" \"+row['Brand Desc']+\" \"+row['Product Size']+\" \"+row['Category']\n",
    "len(documents)\n",
    "# Download latest version of this dataset: https://www.kaggle.com/datasets/sujaykapadnis/products-datasets\n",
    "path2 = kagglehub.dataset_download(\"promptcloud/amazon-product-dataset-2020\")\n",
    "\n",
    "print(\"Path to dataset files:\", path2)\n",
    "try: \n",
    "    prod_data = pd.read_csv(path2+'/home/sdf/'+ os.listdir(path2+'/home/sdf/')[0])\n",
    "    for index, row in prod_data.iterrows():\n",
    "       documents[row['Uniq Id']] = str(row['Product Name'])+\" \"+str(row['Category'])\n",
    "except:\n",
    "    path2 = '/Users/jorgepinzon/.cache/kagglehub/datasets/promptcloud/amazon-product-dataset-2020/versions/1/home/sdf/marketing_sample_for_amazon_com-ecommerce__20200101_20200131__10k_data.csv'\n",
    "    prod_data = pd.read_csv(path2)\n",
    "    for index, row in prod_data.iterrows():\n",
    "        documents[row['Uniq Id']] = str(row['Product Name'])+\" \"+str(row['Category'])\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/p312/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords, wordnet, names\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from symspellpy import SymSpell\n",
    "import pkg_resources\n",
    "import re\n",
    "import torch\n",
    "\n",
    "# Initialize tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize SymSpell for spell-checking\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "# Load male and female names\n",
    "male_names = set(names.words('male.txt'))\n",
    "female_names = set(names.words('female.txt'))\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess(text):\n",
    "    # Lowercase and remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Spell-checking\n",
    "def spell_check(query):\n",
    "    suggestions = sym_spell.lookup_compound(query, max_edit_distance=2)\n",
    "    return suggestions[0].term  # Return the top suggestion\n",
    "\n",
    "# Query expansion with synonyms\n",
    "def expand_query_with_synonyms(query):\n",
    "    tokens = query.split()\n",
    "    expanded_query = []\n",
    "    for token in tokens:\n",
    "        expanded_query.append(token)\n",
    "        synonyms = set()\n",
    "        for syn in wordnet.synsets(token):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name())\n",
    "        expanded_query.extend(list(synonyms)[:2])  # Add top 2 synonyms\n",
    "    return ' '.join(expanded_query)\n",
    "\n",
    "# Gender differentiation\n",
    "def infer_gender(text):\n",
    "    # Tokenize and tag parts of speech\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Extract pronouns and names\n",
    "    pronouns = [word.lower() for word, pos in pos_tags if pos == 'PRP']\n",
    "    names_in_text = [word for word, pos in pos_tags if pos == 'NNP']\n",
    "    \n",
    "    # Count gender-specific pronouns\n",
    "    gender_pronouns = {\n",
    "        \"male\": [\"he\", \"him\", \"his\"],\n",
    "        \"female\": [\"she\", \"her\", \"hers\"]\n",
    "    }\n",
    "    gender_counts = {gender: sum(1 for p in pronouns if p in pronouns_list) for gender, pronouns_list in gender_pronouns.items()}\n",
    "    \n",
    "    # Count gender-specific names\n",
    "    gender_counts[\"male\"] += sum(1 for name in names_in_text if name in male_names)\n",
    "    gender_counts[\"female\"] += sum(1 for name in names_in_text if name in female_names)\n",
    "    \n",
    "    # Infer gender\n",
    "    inferred_gender = max(gender_counts, key=gender_counts.get) if max(gender_counts.values()) > 0 else \"unknown\"\n",
    "    return inferred_gender\n",
    "\n",
    "# Sample documents\n",
    "\n",
    "# Preprocess documents\n",
    "processed_docs = [preprocess(doc) for doc in documents.values()]\n",
    "\n",
    "# Tokenize documents for BM25\n",
    "tokenized_docs = [doc.split() for doc in processed_docs]\n",
    "\n",
    "# Initialize BM25\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "# Load Sentence-BERT model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2').to(device)\n",
    "\n",
    "# Hybrid search function with gender-aware ranking\n",
    "def hybrid_search(query, top_k=5, bm25_weight=0.5, sbert_weight=0.5, gender_weight=0.2):\n",
    "    # Step 1: Spell-check the query\n",
    "    corrected_query = spell_check(query)\n",
    "    print(f\"Corrected Query: {corrected_query}\")\n",
    "    \n",
    "    # Step 2: Infer gender for the query\n",
    "    query_gender = infer_gender(corrected_query)\n",
    "    print(f\"Inferred Gender for Query: {query_gender}\")\n",
    "    \n",
    "    # Step 3: Expand the query with synonyms\n",
    "    expanded_query = expand_query_with_synonyms(corrected_query)\n",
    "    print(f\"Expanded Query: {expanded_query}\")\n",
    "    \n",
    "    # Preprocess the expanded query\n",
    "    processed_query = preprocess(expanded_query)\n",
    "    \n",
    "    # Step 4: BM25 for candidate retrieval\n",
    "    query_tokens = processed_query.split()\n",
    "    bm25_scores = bm25.get_scores(query_tokens)\n",
    "    \n",
    "    # Get top-k candidates based on BM25 scores\n",
    "    candidate_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:top_k]\n",
    "    candidate_docs = [processed_docs[i] for i in candidate_indices]\n",
    "    candidate_ids = [list(documents.keys())[i] for i in candidate_indices]\n",
    "    \n",
    "    # Step 5: Sentence-BERT for re-ranking\n",
    "    candidate_embeddings = model.encode(candidate_docs, device=device)\n",
    "    query_embedding = model.encode([processed_query], device=device)\n",
    "    sbert_scores = cosine_similarity(query_embedding, candidate_embeddings).flatten()\n",
    "    \n",
    "    # Step 6: Infer gender for candidate documents\n",
    "    candidate_genders = [infer_gender(documents[doc_id]) for doc_id in candidate_ids]\n",
    "    \n",
    "    # Step 7: Combine BM25, Sentence-BERT, and gender scores\n",
    "    combined_scores = []\n",
    "    for j, i in enumerate(candidate_indices):\n",
    "        # Base score from BM25 and Sentence-BERT\n",
    "        base_score = (bm25_weight * bm25_scores[i]) + (sbert_weight * sbert_scores[j])\n",
    "        \n",
    "        # Gender score: boost if document gender matches query gender\n",
    "        gender_score = 1.0 if candidate_genders[j] == query_gender else 0.0\n",
    "        \n",
    "        # Combined score\n",
    "        combined_score = base_score + (gender_weight * gender_score)\n",
    "        combined_scores.append(combined_score)\n",
    "    \n",
    "    # Rank documents by combined scores\n",
    "    ranked_docs = sorted(zip(candidate_ids, combined_scores, candidate_genders), key=lambda x: x[1], reverse=True)\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Query: can you suggest a present for alice\n",
      "Inferred Gender for Query: unknown\n",
      "Expanded Query: can force_out fundament you suggest hint evoke a angstrom deoxyadenosine_monophosphate present demo portray for alice\n",
      "\n",
      "Ranked Documents:\n",
      "Document: 435, Score: 9.9272, Inferred Gender: female\n",
      ">>> AJMAL18 - Fragrance-Women mens evoke silver edithimclassi deodorant 200 ml Nan Fragrance-Women\n",
      "Document: bcb528aab71214f7c240b4a695ae6415, Score: 9.6757, Inferred Gender: female\n",
      ">>> Charades Storybook Alice Children's Costume, Small Toys & Games | Dress Up & Pretend Play | Costumes\n",
      "Document: 428, Score: 9.2900, Inferred Gender: female\n",
      ">>> AJMAL11 - Fragrance-Women womens evoke gold edition her edp fruity perfume - 75 ml Nan Fragrance-Women\n",
      "Document: 427, Score: 9.2691, Inferred Gender: female\n",
      ">>> AJMAL10 - Fragrance-Women womens evoke silver edition her edp citrus perfume - 75 ml Nan Fragrance-Women\n",
      "Document: 9eb3927f946ce5b4ca4b57630f7fadbc, Score: 8.1012, Inferred Gender: unknown\n",
      ">>> 2.4Ghz Radio Control 16 Channel Alloy Timber Grab Excavator Rechargeable Sound and Light Demo Remote Control Engineer Truck Toys & Games | Kids' Electronics\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = '''Can you suggest a present for alice? '''\n",
    "results = hybrid_search(query, top_k=5, bm25_weight=1, sbert_weight=1)\n",
    "# Print results\n",
    "print(\"\\nRanked Documents:\")\n",
    "for doc_id, score, gender in results:\n",
    "    print(f\"Document: {doc_id}, Score: {score:.4f}, Inferred Gender: {gender}\")\n",
    "    print(f'>>> {documents[doc_id]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH OUT CHECKING FOR SYNONIMS \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download NLTK stopwords (run once)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text.lower())\n",
    "    tokens = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# # Sample documents\n",
    "# documents = {\n",
    "#     \"doc1\": \"Python is a programming language\",\n",
    "#     \"doc2\": \"Python is used for web development and data science\",\n",
    "#     \"doc3\": \"Natural Language Processing powers search engines\",\n",
    "# }\n",
    "\n",
    "# Preprocess docs\n",
    "processed_docs = [preprocess(doc) for doc in documents.values()]\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_docs)\n",
    "\n",
    "# Search function\n",
    "def search(query):\n",
    "    processed_query = preprocess(query)\n",
    "    query_vec = vectorizer.transform([processed_query])\n",
    "    cos_sim = cosine_similarity(query_vec, tfidf_matrix)\n",
    "    scores = cos_sim.flatten()\n",
    "    ranked_docs = sorted(zip(documents.keys(), scores), key=lambda x: x[1], reverse=True)\n",
    "    return ranked_docs, [doc for doc, score in ranked_docs if score > 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_docs, results = search(\"cozzy shirt\")\n",
    "for key in results: \n",
    "    print(f'''{key}:\\t {documents[key]}''') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from symspellpy import SymSpell\n",
    "import pkg_resources\n",
    "\n",
    "# Initialize SymSpell for spell-check\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "# Text preprocessing\n",
    "def syn_preprocess(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text.lower())\n",
    "    tokens = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Spell-check\n",
    "def spell_check(query):\n",
    "    suggestions = sym_spell.lookup_compound(query, max_edit_distance=2)\n",
    "    return suggestions[0].term\n",
    "\n",
    "# Synonyms\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "def expand_query_with_synonyms(query):\n",
    "    tokens = query.split()\n",
    "    expanded_query = []\n",
    "    for token in tokens:\n",
    "        expanded_query.append(token)\n",
    "        synonyms = get_synonyms(token)\n",
    "        expanded_query.extend(synonyms[:2])  # Add top 2 synonyms\n",
    "    return ' '.join(expanded_query)\n",
    "\n",
    "# Preprocess docs\n",
    "processed_docs = [syn_preprocess(doc) for doc in documents.values()]\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_docs)\n",
    "\n",
    "# Search function\n",
    "def syn_search(query):\n",
    "    # Spell-check\n",
    "    corrected_query = spell_check(query)\n",
    "    print(f\"Corrected Query: {corrected_query}\")\n",
    "    \n",
    "    # Expand with synonyms\n",
    "    expanded_query = expand_query_with_synonyms(corrected_query)\n",
    "    print(f\"Expanded Query: {expanded_query}\")\n",
    "    \n",
    "    # Preprocess query\n",
    "    processed_query = syn_preprocess(expanded_query)\n",
    "    \n",
    "    # Vectorize and compare\n",
    "    query_vec = vectorizer.transform([processed_query])\n",
    "    cos_sim = cosine_similarity(query_vec, tfidf_matrix)\n",
    "    scores = cos_sim.flatten()\n",
    "    \n",
    "    # Rank documents\n",
    "    ranked_docs = sorted(zip(documents.keys(), scores), key=lambda x: x[1], reverse=True)\n",
    "    return ranked_docs, [doc for doc, score in ranked_docs if score >0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, results = syn_search(\"I need a warm t-shirt for a boy, not a perfume, not too big but also not small\")\n",
    "print(len(results))\n",
    "for key in results: \n",
    "    print(f'''{key}:\\t {documents[key]}''') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc, score in scores:\n",
    "    if score > 0.1:\n",
    "        print(f\"Document: {doc}, Score: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from symspellpy import SymSpell\n",
    "import pkg_resources\n",
    "import re\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "\n",
    "# Initialize tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Spell-check\n",
    "def spell_check(query):\n",
    "    suggestions = sym_spell.lookup_compound(query, max_edit_distance=2)\n",
    "    return suggestions[0].term\n",
    "\n",
    "# Query expansion\n",
    "def expand_query_with_synonyms(query):\n",
    "    tokens = query.split()\n",
    "    expanded_query = []\n",
    "    for token in tokens:\n",
    "        expanded_query.append(token)\n",
    "        synonyms = set()\n",
    "        for syn in wordnet.synsets(token):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name())\n",
    "        expanded_query.extend(list(synonyms)[:2])\n",
    "    return ' '.join(expanded_query)\n",
    "# Preprocess documents\n",
    "processed_docs = [preprocess(doc) for doc in documents.values()]\n",
    "\n",
    "# TF-IDF Vectorization with n-grams\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_docs)\n",
    "\n",
    "#BM25\n",
    "# Tokenize documents\n",
    "tokenized_docs = [doc.split() for doc in processed_docs]\n",
    "# Initialize BM25\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "# BERT: \n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "# Load pre-trained Sentence-BERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight and fast model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "# Encode documents in batches\n",
    "def encode_in_batches(texts, model, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        batch_embeddings = model.encode(batch, device=device)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    return embeddings\n",
    "\n",
    "doc_embeddings = encode_in_batches(processed_docs, model, batch_size=32)\n",
    "\n",
    "# Search function\n",
    "def impr_search(query):\n",
    "    # Spell-check\n",
    "    corrected_query = spell_check(query)\n",
    "    print(f\"Corrected Query: {corrected_query}\")\n",
    "    \n",
    "    # Expand with synonyms\n",
    "    expanded_query = expand_query_with_synonyms(corrected_query)\n",
    "    print(f\"Expanded Query: {expanded_query}\")\n",
    "    \n",
    "    # Preprocess query\n",
    "    processed_query = preprocess(expanded_query)\n",
    "    \n",
    "    # Vectorize and compare\n",
    "    query_vec = vectorizer.transform([processed_query])\n",
    "    cos_sim = cosine_similarity(query_vec, tfidf_matrix)\n",
    "    scores = cos_sim.flatten()\n",
    "    \n",
    "    # Rank documents\n",
    "    ranked_docs = sorted(zip(documents.keys(), scores), key=lambda x: x[1], reverse=True)\n",
    "    return ranked_docs, [doc for doc, score in ranked_docs if score > 0]\n",
    "\n",
    "def bmk_search(query):\n",
    "    # Preprocess query\n",
    "    processed_query = preprocess(query)\n",
    "    query_tokens = processed_query.split()\n",
    "    \n",
    "    # Get BM25 scores\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    \n",
    "    # Rank documents\n",
    "    ranked_docs = sorted(zip(documents.keys(), scores), key=lambda x: x[1], reverse=True)\n",
    "    return ranked_docs, [doc for doc, score in ranked_docs if score > 0]\n",
    "\n",
    "def bert_search(query):\n",
    "    # Preprocess query\n",
    "    processed_query = preprocess(query)\n",
    "    \n",
    "    # Encode query\n",
    "    query_embedding = model.encode([processed_query], device=device)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    scores = cosine_similarity(query_embedding, doc_embeddings).flatten()\n",
    "    \n",
    "    # Rank documents\n",
    "    ranked_docs = sorted(zip(documents.keys(), scores), key=lambda x: x[1], reverse=True)\n",
    "    return ranked_docs, [doc for doc, score in ranked_docs if score > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '''Can you suggest a nice cologne for my son? '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_docs, results = impr_search(sentence)\n",
    "print(len(results))\n",
    "for key in results: \n",
    "    print(f'''{key}:\\t {documents[key]}''') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked, results = bmk_search(sentence)\n",
    "print(len(results))\n",
    "for key in results: \n",
    "    print(f'''{key}:\\t {documents[key]}''') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked, results = bert_search(sentence)\n",
    "print(len(results))\n",
    "for key in results: \n",
    "    print(f'''{key}:\\t {documents[key]}''') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from symspellpy import SymSpell\n",
    "import pkg_resources\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data (run once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize SymSpell for spell-checking\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess(text):\n",
    "    # Lowercase and remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Spell-checking\n",
    "def spell_check(query):\n",
    "    suggestions = sym_spell.lookup_compound(query, max_edit_distance=2)\n",
    "    return suggestions[0].term  # Return the top suggestion\n",
    "\n",
    "# Query expansion with synonyms\n",
    "def expand_query_with_synonyms(query):\n",
    "    tokens = query.split()\n",
    "    expanded_query = []\n",
    "    for token in tokens:\n",
    "        expanded_query.append(token)\n",
    "        synonyms = set()\n",
    "        for syn in wordnet.synsets(token):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name())\n",
    "        expanded_query.extend(list(synonyms)[:2])  # Add top 2 synonyms\n",
    "    return ' '.join(expanded_query)\n",
    "\n",
    "# Sample documents\n",
    "# documents = {\n",
    "#     \"doc1\": \"Python is a programming language\",\n",
    "#     \"doc2\": \"Python is used for web development and data science\",\n",
    "#     \"doc3\": \"Natural Language Processing powers search engines\",\n",
    "#     \"doc4\": \"Data science involves Python and machine learning\",\n",
    "#     \"doc5\": \"Web development requires HTML, CSS, and JavaScript\",\n",
    "# }\n",
    "\n",
    "# Preprocess documents\n",
    "processed_docs = [preprocess(doc) for doc in documents.values()]\n",
    "\n",
    "# Tokenize documents for BM25\n",
    "tokenized_docs = [doc.split() for doc in processed_docs]\n",
    "\n",
    "# Initialize BM25\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "# Load Sentence-BERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight model\n",
    "\n",
    "# Hybrid search function\n",
    "def hybrid_search(query, top_k=5, bm25_weight=0.5, sbert_weight=0.5):\n",
    "    # Step 1: Spell-check the query\n",
    "    corrected_query = spell_check(query)\n",
    "    print(f\"Corrected Query: {corrected_query}\")\n",
    "    \n",
    "    # Step 2: Expand the query with synonyms\n",
    "    expanded_query = expand_query_with_synonyms(corrected_query)\n",
    "    print(f\"Expanded Query: {expanded_query}\")\n",
    "    \n",
    "    # Preprocess the expanded query\n",
    "    processed_query = preprocess(expanded_query)\n",
    "    \n",
    "    # Step 3: BM25 for candidate retrieval\n",
    "    query_tokens = processed_query.split()\n",
    "    bm25_scores = bm25.get_scores(query_tokens)\n",
    "    \n",
    "    # Get top-k candidates based on BM25 scores\n",
    "    candidate_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:top_k]\n",
    "    candidate_docs = [processed_docs[i] for i in candidate_indices]\n",
    "    candidate_ids = [list(documents.keys())[i] for i in candidate_indices]\n",
    "    \n",
    "    # Step 4: Sentence-BERT for re-ranking\n",
    "    candidate_embeddings = model.encode(candidate_docs)\n",
    "    query_embedding = model.encode([processed_query])\n",
    "    sbert_scores = cosine_similarity(query_embedding, candidate_embeddings).flatten()\n",
    "    \n",
    "    # Step 5: Combine BM25 and Sentence-BERT scores\n",
    "    combined_scores = [\n",
    "        (bm25_weight * bm25_scores[i]) + (sbert_weight * sbert_scores[j])\n",
    "        for j, i in enumerate(candidate_indices)\n",
    "    ]\n",
    "    \n",
    "    # Rank documents by combined scores\n",
    "    ranked_docs = sorted(zip(candidate_ids, combined_scores), key=lambda x: x[1], reverse=True)\n",
    "    return ranked_docs\n",
    "\n",
    "# Example query\n",
    "query = \"Pythn data scince\"\n",
    "results = hybrid_search(query, top_k=5, bm25_weight=0.5, sbert_weight=0.5)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nRanked Documents:\")\n",
    "for doc_id, score in results:\n",
    "    print(f\"Document: {doc_id}, Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from symspellpy import SymSpell\n",
    "import pkg_resources\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Download NLTK data (run once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize SymSpell for spell-checking\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess(text):\n",
    "    # Lowercase and remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Spell-checking\n",
    "def spell_check(query):\n",
    "    suggestions = sym_spell.lookup_compound(query, max_edit_distance=2)\n",
    "    return suggestions[0].term  # Return the top suggestion\n",
    "\n",
    "# Query expansion with synonyms\n",
    "def expand_query_with_synonyms(query):\n",
    "    tokens = query.split()\n",
    "    expanded_query = []\n",
    "    for token in tokens:\n",
    "        expanded_query.append(token)\n",
    "        synonyms = set()\n",
    "        for syn in wordnet.synsets(token):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name())\n",
    "        expanded_query.extend(list(synonyms)[:2])  # Add top 2 synonyms\n",
    "    return ' '.join(expanded_query)\n",
    "# Preprocess documents\n",
    "processed_docs = [preprocess(doc) for doc in documents.values()]\n",
    "\n",
    "# Tokenize documents for BM25\n",
    "tokenized_docs = [doc.split() for doc in processed_docs]\n",
    "\n",
    "# Initialize BM25\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "# Load Sentence-BERT model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2').to(device)\n",
    "\n",
    "# Encode documents in batches\n",
    "def encode_in_batches(texts, model, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        batch_embeddings = model.encode(batch, device=device)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    return embeddings\n",
    "\n",
    "# Free up memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Encode documents\n",
    "doc_embeddings = encode_in_batches(processed_docs, model, batch_size=32)\n",
    "\n",
    "# Hybrid search function\n",
    "def hybrid_search(query, top_k=5, bm25_weight=0.5, sbert_weight=0.5):\n",
    "    # Step 1: Spell-check the query\n",
    "    corrected_query = spell_check(query)\n",
    "    print(f\"Corrected Query: {corrected_query}\")\n",
    "    \n",
    "    # Step 2: Expand the query with synonyms\n",
    "    expanded_query = expand_query_with_synonyms(corrected_query)\n",
    "    print(f\"Expanded Query: {expanded_query}\")\n",
    "    \n",
    "    # Preprocess the expanded query\n",
    "    processed_query = preprocess(expanded_query)\n",
    "    \n",
    "    # Step 3: BM25 for candidate retrieval\n",
    "    query_tokens = processed_query.split()\n",
    "    bm25_scores = bm25.get_scores(query_tokens)\n",
    "    \n",
    "    # Get top-k candidates based on BM25 scores\n",
    "    candidate_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:top_k]\n",
    "    candidate_docs = [processed_docs[i] for i in candidate_indices]\n",
    "    candidate_ids = [list(documents.keys())[i] for i in candidate_indices]\n",
    "    \n",
    "    # Step 4: Sentence-BERT for re-ranking\n",
    "    candidate_embeddings = model.encode(candidate_docs, device=device)\n",
    "    query_embedding = model.encode([processed_query], device=device)\n",
    "    sbert_scores = cosine_similarity(query_embedding, candidate_embeddings).flatten()\n",
    "    \n",
    "    # Step 5: Combine BM25 and Sentence-BERT scores\n",
    "    combined_scores = [\n",
    "        (bm25_weight * bm25_scores[i]) + (sbert_weight * sbert_scores[j])\n",
    "        for j, i in enumerate(candidate_indices)\n",
    "    ]\n",
    "    \n",
    "    # Rank documents by combined scores\n",
    "    ranked_docs = sorted(zip(candidate_ids, combined_scores), key=lambda x: x[1], reverse=True)\n",
    "    return ranked_docs\n",
    "\n",
    "# Example query\n",
    "query = \"Pythn data scince\"\n",
    "query = '''Can you suggest a nice cologne for my son? '''\n",
    "results = hybrid_search(query, top_k=5, bm25_weight=0.5, sbert_weight=0.5)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nRanked Documents:\")\n",
    "for doc_id, score in results:\n",
    "    print(f\"Document: {doc_id}, Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_id, score in results:\n",
    "    print(f\"Document: {doc_id}, Score: {score:.4f}\")\n",
    "    print(documents[doc_id])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
